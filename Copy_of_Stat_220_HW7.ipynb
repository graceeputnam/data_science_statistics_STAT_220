{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln6IZJWB42ai"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/drbob-richardson/stat220/blob/main/Assignments/Stat_220_HW7.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Khyh7g0bec"
      },
      "source": [
        "**Problem 1**: Consider the data set on bike share counts in Seoul Korea. You can read in the data using\n",
        "\n",
        "\n",
        "\n",
        "bikes = pd.read_csv(\"https://richardson.byu.edu/220/bikes.csv\")\n",
        "\n",
        "Counts is the number of bicycles rented during the lunch hour each day. The continuous predictors are Temperature, Humidity, Wind_speed, Visibility and Rainfall. Seasons is a categorical variable with multiple levels and Holiday is a categorical variable with two levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E1GfcOq06uj"
      },
      "source": [
        "Part a. Split the data into a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MxEAtzXN0-FX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "\n",
        "bikes = pd.read_csv(\"https://richardson.byu.edu/220/bikes.csv\")\n",
        "\n",
        "X = bikes.drop('Count', axis=1)\n",
        "y = bikes['Count']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVJ3d5Bz0lrq"
      },
      "source": [
        "Part b.  Build a linear regression model for the training data using all the predictors in the model with Count as the target variable.  Remove all predictors from the model with a P-Value greater than 0.05. What predictors are left?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OjiL7Mnc0zoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495808f3-49f9-4ca6-cbe9-748c01c2096f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full model:\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Count   R-squared:                       0.510\n",
            "Model:                            OLS   Adj. R-squared:                  0.494\n",
            "Method:                 Least Squares   F-statistic:                     32.61\n",
            "Date:                Thu, 13 Nov 2025   Prob (F-statistic):           6.17e-39\n",
            "Time:                        23:44:51   Log-Likelihood:                -2084.7\n",
            "No. Observations:                 292   AIC:                             4189.\n",
            "Df Residuals:                     282   BIC:                             4226.\n",
            "Df Model:                           9                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "======================================================================================\n",
            "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
            "--------------------------------------------------------------------------------------\n",
            "const               1147.5193    162.217      7.074      0.000     828.209    1466.830\n",
            "Temperature           16.9466      3.483      4.865      0.000      10.091      23.803\n",
            "Humidity              -7.8660      1.311     -5.998      0.000     -10.447      -5.285\n",
            "Wind_speed            -5.9150     19.436     -0.304      0.761     -44.173      32.343\n",
            "Visibility            -0.0540      0.042     -1.288      0.199      -0.136       0.028\n",
            "Rainfall             -46.2164     21.449     -2.155      0.032     -88.436      -3.996\n",
            "Seasons_Spring        -4.0469     53.632     -0.075      0.940    -109.617     101.523\n",
            "Seasons_Summer      -114.0226     66.767     -1.708      0.089    -245.448      17.403\n",
            "Seasons_Winter      -360.0794     81.787     -4.403      0.000    -521.071    -199.088\n",
            "Holiday_No Holiday  -106.1996     94.013     -1.130      0.260    -291.255      78.856\n",
            "==============================================================================\n",
            "Omnibus:                       27.898   Durbin-Watson:                   2.061\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               44.781\n",
            "Skew:                          -0.592   Prob(JB):                     1.89e-10\n",
            "Kurtosis:                       4.509   Cond. No.                     1.56e+04\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.56e+04. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "Keeping: ['Temperature', 'Humidity', 'Rainfall', 'Seasons_Winter']\n",
            "\n",
            "Reduced model:\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Count   R-squared:                       0.497\n",
            "Model:                            OLS   Adj. R-squared:                  0.490\n",
            "Method:                 Least Squares   F-statistic:                     70.91\n",
            "Date:                Thu, 13 Nov 2025   Prob (F-statistic):           1.07e-41\n",
            "Time:                        23:44:51   Log-Likelihood:                -2088.5\n",
            "No. Observations:                 292   AIC:                             4187.\n",
            "Df Residuals:                     287   BIC:                             4205.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==================================================================================\n",
            "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
            "----------------------------------------------------------------------------------\n",
            "const            973.5005     72.872     13.359      0.000     830.069    1116.932\n",
            "Temperature       12.6393      2.507      5.042      0.000       7.705      17.574\n",
            "Humidity          -7.3075      1.157     -6.315      0.000      -9.585      -5.030\n",
            "Rainfall         -42.8929     21.104     -2.032      0.043     -84.430      -1.356\n",
            "Seasons_Winter  -398.2693     68.120     -5.847      0.000    -532.347    -264.192\n",
            "==============================================================================\n",
            "Omnibus:                       17.387   Durbin-Watson:                   2.017\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.340\n",
            "Skew:                          -0.452   Prob(JB):                     8.55e-06\n",
            "Kurtosis:                       4.050   Cond. No.                         267.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_train_dum = pd.get_dummies(X_train, drop_first=True, dtype=float)\n",
        "X_test_dum = pd.get_dummies(X_test, drop_first=True, dtype=float)\n",
        "X_test_dum = X_test_dum.reindex(columns=X_train_dum.columns, fill_value=0)\n",
        "\n",
        "X_with_const = sm.add_constant(X_train_dum)\n",
        "full_model = sm.OLS(y_train, X_with_const).fit()\n",
        "\n",
        "print(\"Full model:\")\n",
        "print(full_model.summary())\n",
        "\n",
        "keep = full_model.pvalues[1:][full_model.pvalues[1:] <= 0.05]\n",
        "\n",
        "print(\"\\nKeeping:\", list(keep.index))\n",
        "\n",
        "X_reduced = X_with_const[['const'] + list(keep.index)]\n",
        "reduced_model = sm.OLS(y_train, X_reduced).fit()\n",
        "\n",
        "print(\"\\nReduced model:\")\n",
        "print(reduced_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWWifv30y28"
      },
      "source": [
        "Part c. Instead of removing all predictors with a P-Value greater than 0.05, remove the largest P-Value and refit, the repeat that process until all the predictors that remain are significant (have P-Values greater than 0.05). What predictors are left in the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vZFq9cqJ0lb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1f0192-87a8-42c3-f5d0-9e2b928178a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backward elimination:\n",
            "  Worst: Seasons_Spring (p=0.9399)\n",
            "  Worst: Wind_speed (p=0.7413)\n",
            "  Worst: Holiday_No Holiday (p=0.2449)\n",
            "  Worst: Visibility (p=0.1887)\n",
            "  Worst: Rainfall (p=0.0610)\n",
            "  Worst: Seasons_Summer (p=0.0284)\n",
            "  Done! All p-values <= 0.05\n",
            "\n",
            "Final predictors: ['Temperature', 'Humidity', 'Seasons_Summer', 'Seasons_Winter']\n",
            "\n",
            "                             OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Count   R-squared:                       0.498\n",
            "Model:                            OLS   Adj. R-squared:                  0.491\n",
            "Method:                 Least Squares   F-statistic:                     71.26\n",
            "Date:                Thu, 13 Nov 2025   Prob (F-statistic):           7.52e-42\n",
            "Time:                        23:44:51   Log-Likelihood:                -2088.1\n",
            "No. Observations:                 292   AIC:                             4186.\n",
            "Df Residuals:                     287   BIC:                             4205.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==================================================================================\n",
            "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
            "----------------------------------------------------------------------------------\n",
            "const            930.7785     78.976     11.786      0.000     775.333    1086.224\n",
            "Temperature       18.4595      3.398      5.432      0.000      11.771      25.148\n",
            "Humidity          -8.1621      1.014     -8.047      0.000     -10.158      -6.166\n",
            "Seasons_Summer  -134.6759     61.147     -2.202      0.028    -255.029     -14.323\n",
            "Seasons_Winter  -326.3614     74.291     -4.393      0.000    -472.585    -180.137\n",
            "==============================================================================\n",
            "Omnibus:                       24.006   Durbin-Watson:                   2.015\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               38.968\n",
            "Skew:                          -0.515   Prob(JB):                     3.45e-09\n",
            "Kurtosis:                       4.464   Cond. No.                         313.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Backward elimination:\")\n",
        "\n",
        "current_features = list(X_train_dum.columns)\n",
        "\n",
        "while True:\n",
        "    X_curr = sm.add_constant(X_train_dum[current_features])\n",
        "    mod = sm.OLS(y_train, X_curr).fit()\n",
        "\n",
        "    p_vals = mod.pvalues[1:]\n",
        "\n",
        "    max_p = p_vals.max()\n",
        "    worst = p_vals.idxmax()\n",
        "\n",
        "    print(f\"  Worst: {worst} (p={max_p:.4f})\")\n",
        "\n",
        "    if max_p <= 0.05:\n",
        "        print(\"  Done! All p-values <= 0.05\")\n",
        "        break\n",
        "\n",
        "    current_features.remove(worst)\n",
        "\n",
        "X_train_final = sm.add_constant(X_train_dum[current_features])\n",
        "model_1c = sm.OLS(y_train, X_train_final).fit()\n",
        "\n",
        "print(\"\\nFinal predictors:\", current_features)\n",
        "print(\"\\n\", model_1c.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIHdq-K-1IPm"
      },
      "source": [
        "Part d. Regardless of whether or not you got the same set of predictors in problems 1 and 2, the two approaches can potentially give different results. Explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svNRW-7Q1L4S"
      },
      "source": [
        "Each time you remove a feature it could potentially impact the other features and their relationships to one another. A great exmpale we talked about in class was age and years since graduation. These variables are kind of similar becuase they both involve age and the older someone is the longer it has been since they have graduated. In that case taking out one of the variables might fix the problem even thought they both might have had a large p-value to begin with. If you take out one of them it might improve the p-value of the other. This is why it is best to remove one feature at a time until you get to where you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsbe_ZYj3msg"
      },
      "source": [
        "Part e. Find the out of sample MSE for both the model with all predictors, the model with all variables with p values above 0.05 removed, and the model with variables removed 1 at a time. which model is best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fYOVo21a30Kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7c0bd4-14fb-4a4b-b6df-28b05397451a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUT-OF-SAMPLE MSE COMPARISON\n",
            "\n",
            "\n",
            "1. Full Model (all predictors):\n",
            "   MSE = 77830.43\n",
            "\n",
            "2. Part b Model (remove all p>0.05 at once):\n",
            "   Predictors: ['Temperature', 'Humidity', 'Rainfall', 'Seasons_Winter']\n",
            "   MSE = 72715.22\n",
            "\n",
            "3. Part c Model (backward elimination):\n",
            "   Predictors: ['Temperature', 'Humidity', 'Seasons_Summer', 'Seasons_Winter']\n",
            "   MSE = 76984.62\n",
            "\n",
            "** BEST MODEL: Part b Model with MSE = 72715.22 **\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_test_with_const_full = sm.add_constant(X_test_dummies)\n",
        "\n",
        "y_pred_full = model_full.predict(X_test_with_const_full)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "X_test_1b = X_test_with_const_full[['const'] + significant_features]\n",
        "y_pred_1b = model_1b.predict(X_test_1b)\n",
        "mse_1b = mean_squared_error(y_test, y_pred_1b)\n",
        "\n",
        "X_test_1c = X_test_with_const_full[['const'] + current_features]\n",
        "y_pred_1c = model_1c.predict(X_test_1c)\n",
        "mse_1c = mean_squared_error(y_test, y_pred_1c)\n",
        "\n",
        "\n",
        "print(\"OUT-OF-SAMPLE MSE COMPARISON\")\n",
        "print()\n",
        "print(f\"\\n1. Full Model (all predictors):\")\n",
        "print(f\"   MSE = {mse_full:.2f}\")\n",
        "\n",
        "print(f\"\\n2. Part b Model (remove all p>0.05 at once):\")\n",
        "print(f\"   Predictors: {significant_features}\")\n",
        "print(f\"   MSE = {mse_1b:.2f}\")\n",
        "\n",
        "print(f\"\\n3. Part c Model (backward elimination):\")\n",
        "print(f\"   Predictors: {current_features}\")\n",
        "print(f\"   MSE = {mse_1c:.2f}\")\n",
        "\n",
        "best_model = min([(mse_full, \"Full Model\"), (mse_1b, \"Part b Model\"), (mse_1c, \"Part c Model\")], key=lambda x: x[0])\n",
        "print(f\"\\n** BEST MODEL: {best_model[1]} with MSE = {best_model[0]:.2f} **\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PenEv4Y21Sqy"
      },
      "source": [
        "**Problem 2** Use the same data as above and the same train-test split. Build a regression tree with a maximum depth of 2. Find the out of sample MSE.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phtymO_y3Jhd"
      },
      "source": [
        "Part a. Build a regression tree with a maximum depth of 2. Find the out of sample MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9KemnZ5O1M-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df11be9f-a7d3-48ec-b447-2f640f98453a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 2a: Regression Tree with max_depth=2\n",
            "Out-of-sample MSE: 63280.79\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tree_depth2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_depth2.fit(X_train_dummies, y_train)\n",
        "\n",
        "y_pred_tree2 = tree_depth2.predict(X_test_dummies)\n",
        "\n",
        "mse_tree2 = mean_squared_error(y_test, y_pred_tree2)\n",
        "\n",
        "print(\"Problem 2a: Regression Tree with max_depth=2\")\n",
        "print(f\"Out-of-sample MSE: {mse_tree2:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaSbRMvT3PM7"
      },
      "source": [
        "Part b. Increase the depth to 3, 4, 5, and 6. Check the out of sample MSE for each and report them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yJXx71YF0Y2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5ae481-bdc1-4e1f-e9aa-a0a8ac9d3b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth = 3: Out-of-sample MSE = 71484.28\n",
            "Depth = 4: Out-of-sample MSE = 64447.18\n",
            "Depth = 5: Out-of-sample MSE = 66266.62\n",
            "Depth = 6: Out-of-sample MSE = 86835.98\n",
            "\n",
            "All depths tested:\n",
            "  Depth 2: MSE = 63280.79\n",
            "  Depth 3: MSE = 71484.28\n",
            "  Depth 4: MSE = 64447.18\n",
            "  Depth 5: MSE = 66266.62\n",
            "  Depth 6: MSE = 86835.98\n",
            "\n",
            "** BEST DEPTH: 2 with MSE = 63280.79 **\n"
          ]
        }
      ],
      "source": [
        "\n",
        "depths = [3, 4, 5, 6]\n",
        "mse_results = {}\n",
        "\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train_dummies, y_train)\n",
        "\n",
        "    y_pred = tree.predict(X_test_dummies)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_results[depth] = mse\n",
        "\n",
        "    print(f\"Depth = {depth}: Out-of-sample MSE = {mse:.2f}\")\n",
        "\n",
        "all_mse = {2: mse_tree2}\n",
        "all_mse.update(mse_results)\n",
        "\n",
        "print(\"\\nAll depths tested:\")\n",
        "for depth in sorted(all_mse.keys()):\n",
        "    print(f\"  Depth {depth}: MSE = {all_mse[depth]:.2f}\")\n",
        "\n",
        "best_depth = min(all_mse, key=all_mse.get)\n",
        "best_mse = all_mse[best_depth]\n",
        "\n",
        "print()\n",
        "print(f\"** BEST DEPTH: {best_depth} with MSE = {best_mse:.2f} **\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyiLHtBA3XGs"
      },
      "source": [
        "Part c. Based on the out of sample MSE, which depth is best?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the sample MSE, the best depth for this particular model is 2. The MSE is about 1000 lower than the closest competitor with the depth of 2. I love that there are hyperparameters that we can play with to understand the data and build better models. I think that it is important to understand these meterics and how they can help us choose better models so we can create good models that can help people in the real world!"
      ],
      "metadata": {
        "id": "99uaz5h_hjB8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RGh2Bgb3cgg"
      },
      "source": [
        "**Problem 3** Explain why using out of sample metrics is important for finding the best model as opposed to using in sample metrics. Out of all the models, both regression tree and linear regression models, which is the best model using out-of-sample MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P18h6GLG4j0n"
      },
      "source": [
        "Out of sample metrics are important for finding the best model compared to in sample metrics. They are the best becuase they will provide us with a real opportunity to test our model on the test data that it has never seen before. It is a great way to reflect how the model will perform in real life on unseen data.\n",
        "\n",
        "Out of all of the models, the best model using out of smaple MSE is the regression tree with the depth of 2. With an MSE of 63,281, this model beat all the others which consisted of MSE values similar to 77,830, 72,715, and 76,985. All in all, in this case the simple tree structure ended up being the best model for the situation and resulting in the most accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-yQ1sy5Kwn"
      },
      "source": [
        "**Problem 4**: A store with an online presence collects revenue data by month. This data can be found at [richardson.byu.edu/220/revenue_data.csv](https://richardson.byu.edu/220/revenue_data.csv). The variable MonthlyRevenue is the target variable. Money spent on ads (AdSpend), site traffic (AvgTraffic), and discount rates (DiscountRate) are the variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhWkmwRl7nQD"
      },
      "source": [
        "Part a. Split this data into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NGLrK4ae7woq"
      },
      "outputs": [],
      "source": [
        "\n",
        "revenue = pd.read_csv(\"https://richardson.byu.edu/220/revenue_data.csv\")\n",
        "\n",
        "X_rev = revenue.drop('MonthlyRevenue', axis=1)\n",
        "y_rev = revenue['MonthlyRevenue']\n",
        "\n",
        "X_rev_train, X_rev_test, y_rev_train, y_rev_test = train_test_split(X_rev, y_rev, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtllUwy47w6w"
      },
      "source": [
        "Part b. Fit a linear regression model on the training set. Report the p-values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jOYiyRu37_k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cf0892-f6be-497e-b9cd-0b3c8f66b129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:         MonthlyRevenue   R-squared:                       0.707\n",
            "Model:                            OLS   Adj. R-squared:                  0.702\n",
            "Method:                 Least Squares   F-statistic:                     125.7\n",
            "Date:                Thu, 13 Nov 2025   Prob (F-statistic):           2.01e-41\n",
            "Time:                        23:44:52   Log-Likelihood:                -599.10\n",
            "No. Observations:                 160   AIC:                             1206.\n",
            "Df Residuals:                     156   BIC:                             1218.\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "================================================================================\n",
            "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
            "--------------------------------------------------------------------------------\n",
            "const           50.7875      6.977      7.279      0.000      37.005      64.570\n",
            "AdSpend          3.2007      0.177     18.107      0.000       2.851       3.550\n",
            "AvgTraffic       0.0033      0.001      3.800      0.000       0.002       0.005\n",
            "DiscountRate    -0.5661      0.415     -1.364      0.175      -1.386       0.254\n",
            "==============================================================================\n",
            "Omnibus:                        0.224   Durbin-Watson:                   1.962\n",
            "Prob(Omnibus):                  0.894   Jarque-Bera (JB):                0.388\n",
            "Skew:                           0.024   Prob(JB):                        0.824\n",
            "Kurtosis:                       2.764   Cond. No.                     4.40e+04\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 4.4e+04. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            "P-values for each variable:\n",
            "  AdSpend: p-value = 0.000000\n",
            "  AvgTraffic: p-value = 0.000207\n",
            "  DiscountRate: p-value = 0.174614\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_rev_train_const = sm.add_constant(X_rev_train)\n",
        "\n",
        "revenue_model = sm.OLS(y_rev_train, X_rev_train_const).fit()\n",
        "\n",
        "print(revenue_model.summary())\n",
        "\n",
        "print(\"\\nP-values for each variable:\")\n",
        "for var, pval in revenue_model.pvalues[1:].items():\n",
        "    print(f\"  {var}: p-value = {pval:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY4UPmz28AeI"
      },
      "source": [
        "Part c. Interpret the p-value for AdSpend in the context of the problem. What does the value of that p-value imply for the relationship between these variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UNlDXTY883p"
      },
      "source": [
        "The p-value for AdSpend is 0 which means it is highly statistically significant. This super small p-value helps us understand that there is a super strong relationship between AdSpend and Monthly Revenue. With a p-value of 0 you can be pretty sure that this relationship is not just random chance and that if you spend more money on Ads you will increase your monthly revenue. This can help in a business context to know that advertising is working and is a good way to invest in the company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eDS2aIV89RJ"
      },
      "source": [
        "Part d. Interpret the p-value for DiscountRate in the context of the problem. What does the value of that p-value imply for the relationship between these variables."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value for DiscountRate is 0.175, which is greater than 0.05. This means the relationship between DiscountRate and MonthlyRevenue is not statistically significant. Basically, we don't have enough evidence to say that discount rates actually affect revenue - the effect we see could just be random chance."
      ],
      "metadata": {
        "id": "RWXsebQ3k3_N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5q_zs-X9FRL"
      },
      "source": [
        "**Problem 5** Using the same data as problem 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJDAvwyt9Iel"
      },
      "source": [
        "Part a. Build three regression tree models on the training data set with a max depths of 2, 3, and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9ssk-6MA9UeV"
      },
      "outputs": [],
      "source": [
        "\n",
        "tree_models = {}\n",
        "depths = [2, 3, 5]\n",
        "\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_rev_train, y_rev_train)\n",
        "    tree_models[depth] = tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-XFtKLm9VlP"
      },
      "source": [
        "Part b. Find the in sample and out of sample R^2 for all three models. (you shouold have 6 R^2 in total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ezOPBzyi9hbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27cb1b9-3d8d-479a-d88b-172ea84413ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Depth = 2:\n",
            "  In-sample R² (training):  0.6327\n",
            "  Out-of-sample R² (test):  0.6226\n",
            "\n",
            "Depth = 3:\n",
            "  In-sample R² (training):  0.7236\n",
            "  Out-of-sample R² (test):  0.6404\n",
            "\n",
            "Depth = 5:\n",
            "  In-sample R² (training):  0.8661\n",
            "  Out-of-sample R² (test):  0.5100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for depth in [2, 3, 5]:\n",
        "    tree = tree_models[depth]\n",
        "\n",
        "    y_train_pred = tree.predict(X_rev_train)\n",
        "    r2_train = r2_score(y_rev_train, y_train_pred)\n",
        "\n",
        "    y_test_pred = tree.predict(X_rev_test)\n",
        "    r2_test = r2_score(y_rev_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\nDepth = {depth}:\")\n",
        "    print(f\"  In-sample R² (training):  {r2_train:.4f}\")\n",
        "    print(f\"  Out-of-sample R² (test):  {r2_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlBMD1w9hyS"
      },
      "source": [
        "Part c. Use these R^2 values in terms of\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "detecting to detect any underfitting or overfitting in the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00sDBgVW-LBx"
      },
      "source": [
        "\n",
        "Looking at the R² values, depth 2 has in-sample R² of 0.63 and out-of-sample R² of 0.62, depth 3 has 0.72 and 0.64, and depth 5 has 0.87 and 0.51. Depth 2 is clearly the best model because it has the smallest gap between training and test performance and actually gets the best out-of-sample score. Depth 5 has a lot of overfitting. It fits training data great but does terrible on test data becuase it is overfitting badly."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}